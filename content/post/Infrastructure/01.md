---
title: "fluentd日志丢失故障记录及GKE日志轮转分析"
author: "PengHui Yang"
date: "2019-08-15"
weight: 10
tags: [
  "EFK"
]
archives: ["201908"]
---
发现业务log丢失，查看EFK三大组件均正常运行，fluentd CPU利用率较高，无ERROR级别log。本文记录下故障处理过程以及日志轮转对fluentd的影响<!--more-->
---

### 一、log收集方案（EFK）

1. pod打印log到stdout、stderr。docker守护进程将其持久化到`/var/lib/docker`目录，kubernetes将其软链接到`/var/lib/containers`.

2. fluentd作为DeamonSet被部署在每个node上，并tail `/var/lib/containers`整个目录，格式转换后推送到Elasticsearch和GCS.

3. Kibana从Elasticsearch读取log展示.


这个过程中，fluentd需要维护一个缓冲区，按照官方文档说法，流式传输数据，buffer overflow_action应配置为默认值`throw_exception`。log收集应当属于流式传输。

```
overflow_action
Control the buffer behaviour when the queue becomes full. 3 modes are supported:

1. throw_exception
This is default mode. This mode raises BufferOverflowError exception to input plugin. How handle BufferOverflowError depends on input plugins, e.g. tail input stops reading new lines, forward input returns an error to forward output. This action fits for streaming manner.

2. block
This mode stops input plugin thread until buffer full is resolved. This action is good for batch-like use-case.
We don't recommend to use block action to avoid BufferOverflowError. Please consider improving destination setting to resolve BufferOverflowError or use @ERROR label for routing overflowed events to another backup destination(or secondary with lower retry_limit). If you hit BufferOverflowError frequently, it means your destination capacity is insufficient for your traffic.

3. drop_oldest_chunk
This mode drops oldest chunks. This mode is useful for monitoring system destinations. For monitoring, newer events are important than older.
```

in_tail插件会根据异常进行以下动作：

```
What happens when in_tail receives BufferOverflowError?
in_tail stops reading new lines and pos file update until BufferOverflowError is resolved. After resolved BufferOverflowError, restart emitting new lines and pos file update.
```

### 二、问题排查

设置log级别debug
```
<store ignore_error>
   @id elasticsearch
   @type elasticsearch
   @log_level debug
  ...
</store>
```
发现如下关键log`cluster_block_exception`
```
2019-05-31 11:39:12 +0000 [debug]: [elasticsearch] Indexed (op = index), 21 successes, 9020 cluster_block_exception
2019-05-31 11:39:13 +0000 [debug]: [elasticsearch] Created new chunk chunk_id="58a2d76bfad8b58d43884fcdc488c11b" metadata=#<struct Fluent::Plugin::Buffer::Metadata timekey=nil, tag="microservice.access.*", variables=nil>
2019-05-31 11:39:14 +0000 [warn]: [elasticsearch] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=42.28314201813191 slow_flush_log_threshold=20.0 plugin_id="elasticsearch"
2019-05-31 11:39:16 +0000 [debug]: [elasticsearch] Created new chunk chunk_id="58a2d76e8b62dbc65309e1bce7f0caa2" metadata=#<struct Fluent::Plugin::Buffer::Metadata timekey=nil, tag="microservice.app.*", variables=nil>
2019-05-31 11:39:16 +0000 [debug]: [elasticsearch] Indexed (op = index), 76 successes, 6462 cluster_block_exception
2019-05-31 11:39:18 +0000 [warn]: [elasticsearch] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=30.385294652543962 slow_flush_log_threshold=20.0 plugin_id="elasticsearch"
2019-05-31 11:39:18 +0000 [debug]: [elasticsearch] Created new chunk chunk_id="58a2d7707069fbe07bc313e68a476fbb" metadata=#<struct Fluent::Plugin::Buffer::Metadata timekey=nil, tag="microservice.access.*", variables=nil>
2019-05-31 11:39:20 +0000 [warn]: buffer flush took longer time than slow_flush_log_threshold: elapsed_time=22.49672028142959 slow_flush_log_threshold=20.0 plugin_id="object:3fe649151df4"
2019-05-31 11:39:20 +0000 [warn]: [elasticsearch] buffer flush took longer time than slow_flush_log_threshold: elapsed_time=51.35625608731061 slow_flush_log_threshold=20.0 plugin_id="elasticsearch"
2019-05-31 11:39:20 +0000 [debug]: [elasticsearch] Indexed (op = index), 21 successes, 6888 cluster_block_exception
```

检查配置，找到原因

```
GET _settings
# 执行结果
"logstash-2019.05.28": {
    "settings": {
      "index": {
        "number_of_shards": "5",
        "blocks": {
          "read_only_allow_delete": "true"
        },
        "provided_name": "logstash-2019.05.28",
        "creation_date": "1559088651380",
        "number_of_replicas": "0",
        "uuid": "bFgUf2e_SYSPmKECdgon4Q",
        "version": {
          "created": "6040299"
        }
      }
    }
  }
```

官方文档解释：
https://www.elastic.co/guide/en/elasticsearch/reference/6.4/disk-allocator.html

简单讲就是磁盘空间不足导致es自动设置索引为只读(允许删除)了，导致28号及之前的索引被锁，log推不上去。在删除部分索引后，新建的索引正常，但还是慢。

临时性解决方法：https://stackoverflow.com/questions/50609417/elasticsearch-error-cluster-block-exception-forbidden-12-index-read-only-all

```
PUT _settings
{
    "index": {
      "blocks": {
      "read_only_allow_delete": "false"
      }
    }
}
```

未来的改进：磁盘扩容或者增加es节点

### 三、既然设置了overflow_action为何还会丢log

通过https://murat.io/notes/2017/logrotate-container-logs-in-google-container-engine.html 得知，GKE默认`maximum file size`和`maximum number of files`是`10mb`和`5`。当log迟迟无法推上ElasticSearch时，pod的`log已经发生了轮转`，由于fluentd tail插件读取的实际文件为软链接，导致即使BufferOverflowError问题解决，也会丢失log。

以xxx服务为例，看到线上环境log目录下
```
gke-production-larger-pool-1b83bd61-5ml9 /var/log/pods/a2aa1221-ba7a-11e9-bd74-42010a8a024f/xxx # ls -l
total 4
lrwxrwxrwx 1 root root 165 Aug  9 07:52 0.log -> /var/lib/docker/containers/84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55/84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log

gke-production-larger-pool-1b83bd61-5ml9 /var/lib/docker/containers/84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55 # ls -l
total 47768
-r-------- 1 root root  8824513 Aug 13 08:35 84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log
-r-------- 1 root root 10036892 Aug 13 07:24 84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log.1
-r-------- 1 root root 10009127 Aug 13 06:10 84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log.2
-r-------- 1 root root 10000659 Aug 13 04:07 84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log.3
-r-------- 1 root root 10000507 Aug 13 04:07 84f23be2470f06614a8a070037385927bfa706bb03e67b314d5b380a3a936f55-json.log.4
drwx------ 2 root root     4096 Aug  9 07:52 checkpoints
-rw-r--r-- 1 root root    20184 Aug  9 07:52 config.v2.json
-rw-r--r-- 1 root root     1994 Aug  9 07:52 hostconfig.json
```

解决问题的方法是修改GCE的`instance template`中`kube-env`部分，添加参数
```
DOCKER_LOG_MAX_SIZE: Xmb
DOCKER_LOG_MAX_FILE: Y
```
具体操作可参考: https://stackoverflow.com/questions/49136527/accessing-kubelet-settings-on-gke-to-fix-nodehasdiskpressure

### 四、日志轮转对fluentd的影响

fluentd已经考虑到日志发生轮转的情况，in_tail插件提供了`rotate_wait`参数，默认值是5s。tail插件内部保存了目标文件的inode号码，当检测到发生log轮转后，在该时间内继续保留对旧文件的引用，以完成对旧文件数据的处理，从而防止数据的丢失。

日志轮转程序通常会有两种模式处理老旧日志：

1. 一是拷贝所有旧文件数据到新文件，然后清空老文件，由于旧文件的inode没有发生变化，所以应用程序继续往里写。但这样可能会有丢失数据的风险，即从拷贝完老数据到清空旧文件这段时间所产生的log会丢失。

2. 二是重命名旧文件，新建空文件，由于inode变化了，所以须为应用程序发送reopen信号使其往新文件中写，这就要求应用程序提供这样的接口，否则需要重启。

tail插件对日志转换的处理只适应于方案二，否则inode没有变化，无法感知到发生log轮转，无法从新文件开始读。以logrotate为例，则不应该具有`nocreate`参数。

另外，在对tail插件path路径的配置，应注意谨慎使用‘*’，否则可能应为该目录下发生日志轮转，增加新的文件，导致log重复读取。推荐使用软链接形式。

参考文档：

https://docs.fluentd.org/output

https://docs.fluentd.org/input/tail

https://medium.com/ninjavan-tech/logging-with-efk-in-gke-e11676f781e6

https://docs.fluentd.org/v/0.12/input/tail
